Error processing instruction process_bundle-311331-335753. Original traceback is Traceback (most recent call last): File "apache_beam/runners/common.py", line 1418, in apache_beam.runners.common.DoFnRunner.process File "apache_beam/runners/common.py", line 838, in apache_beam.runners.common.PerWindowInvoker.invoke_process File "apache_beam/runners/common.py", line 984, in apache_beam.runners.common.PerWindowInvoker._invoke_process_per_window File "C:\Users\b0s05xh\PycharmProjects\gcp-custom-dataflow\venv\lib\site-packages\apache_beam\transforms\util.py", line 1096, in process buffering_timer.set(clock() + max_buffering_duration_secs) File "/usr/local/lib/python3.10/site-packages/apache_beam/runners/worker/bundle_processor.py", line 501, in add accumulator = self._read_accumulator(False) File "/usr/local/lib/python3.10/site-packages/apache_beam/runners/worker/bundle_processor.py", line 484, in _read_accumulator merged_accumulator = self._combinefn.merge_accumulators( File "/usr/local/lib/python3.10/site-packages/apache_beam/transforms/combiners.py", line 185, in merge_accumulators return sum(accumulators) File "/usr/local/lib/python3.10/site-packages/apache_beam/runners/worker/bundle_processor.py", line 533, in __iter__ for elem in self.first: File "/usr/local/lib/python3.10/site-packages/apache_beam/runners/worker/sdk_worker.py", line 1250, in _lazy_iterator self._underlying.get_raw(state_key, continuation_token)) File "/usr/local/lib/python3.10/site-packages/apache_beam/runners/worker/sdk_worker.py", line 1059, in get_raw response = self._blocking_request( File "/usr/local/lib/python3.10/site-packages/apache_beam/runners/worker/sdk_worker.py", line 1104, in _blocking_request raise RuntimeError(response.error) RuntimeError: INTERNAL: The work item requesting state read is no longer valid on the backend. The work has already completed or will be retried. This is expected during autoscaling events. [type.googleapis.com/util.MessageSetPayload='[dist_proc.dax.internal.TrailProto] { trail_point { source_file_loc { filepath: "dist_proc/windmill/client/streaming_rpc_client.cc" line: 761 } } }'] During handling of the above exception, another exception occurred: Traceback (most recent call last): File "/usr/local/lib/python3.10/site-packages/apache_beam/runners/worker/sdk_worker.py", line 292, in _execute response = task() File "/usr/local/lib/python3.10/site-packages/apache_beam/runners/worker/sdk_worker.py", line 365, in <lambda> lambda: self.create_worker().do_instruction(request), request) File "/usr/local/lib/python3.10/site-packages/apache_beam/runners/worker/sdk_worker.py", line 624, in do_instruction return getattr(self, request_type)( File "/usr/local/lib/python3.10/site-packages/apache_beam/runners/worker/sdk_worker.py", line 662, in process_bundle bundle_processor.process_bundle(instruction_id)) File "/usr/local/lib/python3.10/site-packages/apache_beam/runners/worker/bundle_processor.py", line 1062, in process_bundle input_op_by_transform_id[element.transform_id].process_encoded( File "/usr/local/lib/python3.10/site-packages/apache_beam/runners/worker/bundle_processor.py", line 232, in process_encoded self.output(decoded_value) File "apache_beam/runners/worker/operations.py", line 526, in apache_beam.runners.worker.operations.Operation.output File "apache_beam/runners/worker/operations.py", line 528, in apache_beam.runners.worker.operations.Operation.output File "apache_beam/runners/worker/operations.py", line 237, in apache_beam.runners.worker.operations.SingletonElementConsumerSet.receive File "apache_beam/runners/worker/operations.py", line 240, in apache_beam.runners.worker.operations.SingletonElementConsumerSet.receive File "apache_beam/runners/worker/operations.py", line 907, in apache_beam.runners.worker.operations.DoOperation.process File "apache_beam/runners/worker/operations.py", line 908, in apache_beam.runners.worker.operations.DoOperation.process File "apache_beam/runners/common.py", line 1420, in apache_beam.runners.common.DoFnRunner.process File "apache_beam/runners/common.py", line 1508, in apache_beam.runners.common.DoFnRunner._reraise_augmented File "apache_beam/runners/common.py", line 1418, in apache_beam.runners.common.DoFnRunner.process File "apache_beam/runners/common.py", line 838, in apache_beam.runners.common.PerWindowInvoker.invoke_process File "apache_beam/runners/common.py", line 984, in apache_beam.runners.common.PerWindowInvoker._invoke_process_per_window File "C:\Users\b0s05xh\PycharmProjects\gcp-custom-dataflow\venv\lib\site-packages\apache_beam\transforms\util.py", line 1096, in process buffering_timer.set(clock() + max_buffering_duration_secs) File "/usr/local/lib/python3.10/site-packages/apache_beam/runners/worker/bundle_processor.py", line 501, in add accumulator = self._read_accumulator(False) File "/usr/local/lib/python3.10/site-packages/apache_beam/runners/worker/bundle_processor.py", line 484, in _read_accumulator merged_accumulator = self._combinefn.merge_accumulators( File "/usr/local/lib/python3.10/site-packages/apache_beam/transforms/combiners.py", line 185, in merge_accumulators return sum(accumulators) File "/usr/local/lib/python3.10/site-packages/apache_beam/runners/worker/bundle_processor.py", line 533, in __iter__ for elem in self.first: File "/usr/local/lib/python3.10/site-packages/apache_beam/runners/worker/sdk_worker.py", line 1250, in _lazy_iterator self._underlying.get_raw(state_key, continuation_token)) File "/usr/local/lib/python3.10/site-packages/apache_beam/runners/worker/sdk_worker.py", line 1059, in get_raw response = self._blocking_request( File "/usr/local/lib/python3.10/site-packages/apache_beam/runners/worker/sdk_worker.py", line 1104, in _blocking_request raise RuntimeError(response.error) RuntimeError: INTERNAL: The work item requesting state read is no longer valid on the backend. The work has already completed or will be retried. This is expected during autoscaling events. [type.googleapis.com/util.MessageSetPayload='[dist_proc.dax.internal.TrailProto] { trail_point { source_file_loc { filepath: "dist_proc/windmill/client/streaming_rpc_client.cc" line: 761 } } }'] [while running 'Write to BigQuery WM Stores/_StreamToBigQuery/WithAutoSharding/GroupIntoBatches/ParDo(_GroupIntoBatchesDoFn)-ptransform-55']

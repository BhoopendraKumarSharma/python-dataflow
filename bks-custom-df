import csv
import logging
from io import StringIO
from datetime import datetime
import os

import apache_beam as beam
from apache_beam.io import fileio
from apache_beam.io.gcp.internal.clients import bigquery
from apache_beam.options.pipeline_options import PipelineOptions, StandardOptions
from apache_beam.transforms import trigger
from apache_beam.transforms.window import FixedWindows

os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = "vivid-argon-376508-f773dd3b2532.json"

def parse_csv(line):
    # Read the CSV line using the csv module
    csv_reader = csv.reader(StringIO(line), delimiter=',')
    for row in csv_reader:
        # Extract the desired fields and create a dictionary
        record = {
            "dhc_id": row[0],
            "domain_id": row[1],
            "domain": row[2],
            "full_name": row[4],
            "city": row[16],
            "state": row[17],
        }

        return record


def file_prefix(file_info):
    now = datetime.now()
    date_string = now.strftime("%Y-%m-%d")
    original_file_name = file_info.file_metadata.path.split("/")[-1]
    return f"{date_string}/{original_file_name}"


def run(argv=None):
    # Hardcode your input parameters here
    input_path = "gs://bks-pipeline-test/*"
    output_table = "vivid-argon-376508:bksdatasets001.bks-df-test"
    destination_bucket = "gs://df-data-output/"
    poll_interval = 60
    window_size = 300

    pipeline_options = PipelineOptions(argv)
    pipeline_options.view_as(
        beam.options.pipeline_options.GoogleCloudOptions).region = "us-central1"
    pipeline_options.view_as(StandardOptions).streaming = True

    with beam.Pipeline(options=pipeline_options) as p:
        input_file_patterns = p | "Create File Patterns" >> beam.Create([input_path])

        csv_lines = (
                input_file_patterns
                | "Apply Windowing" >> beam.WindowInto(FixedWindows(window_size),
                                                       trigger=trigger.Repeatedly(trigger.AfterCount(poll_interval)),
                                                       accumulation_mode=trigger.AccumulationMode.DISCARDING)
                | "ReadAll from GCS" >> beam.io.ReadAllFromText()
                | "ParseCSV" >> beam.Map(parse_csv)
        )

        bq_output = (
                csv_lines
                | "WriteToBigQuery" >> beam.io.WriteToBigQuery(
            table=output_table,
            schema='dhc_id:INTEGER,domain_id:INTEGER,domain:STRING,city:STRING,state:STRING',
            create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,
            write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND,
        )
        )

        move_files = (
                input_file_patterns
                | "Apply Windowing for Moving Files" >> beam.WindowInto(FixedWindows(window_size),
                                                                        trigger=trigger.Repeatedly(
                                                                            trigger.AfterCount(poll_interval)),
                                                                        accumulation_mode=trigger.AccumulationMode.DISCARDING)
                | "MatchAllFiles2" >> fileio.MatchAll()
                | "ReadMatches2" >> fileio.ReadMatches()
                | "WriteToGCS" >> fileio.WriteToFiles(
            path=destination_bucket,
            destination=file_prefix,
            sink=lambda _, temp_dir: fileio.FileWriter(temp_dir),
        )
        )


if __name__ == "__main__":
    logging.getLogger().setLevel(logging.INFO)
    run()

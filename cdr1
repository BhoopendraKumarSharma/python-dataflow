import apache_beam as beam
from apache_beam.options.pipeline_options import PipelineOptions
from apache_beam.transforms.window import FixedWindows
from datetime import datetime
import csv
from io import StringIO
import logging
import os

os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'path/to/credentials.json'


def parse_csv(line):
    csv_reader = csv.reader(StringIO(line), delimiter=',')
    next(csv_reader)  # skip header row
    for row in csv_reader:
        record = {
            'dhc_id': int(row[0]),
            'domain_id': int(row[1]),
            'domain': row[2],
            'full_name': row[4],
            'city': row[16],
            'state': row[17]
        }
        yield record


def run():
    input_path = 'gs://input-bucket/*.csv'
    output_table = 'my-project:my_dataset.my_table'
    output_path = 'gs://output-bucket/output_file.csv'
    poll_interval = 60
    window_size = 300

    pipeline_options = PipelineOptions()
    pipeline_options.view_as(
        beam.options.pipeline_options.GoogleCloudOptions).project = 'my-project'
    pipeline_options.view_as(
        beam.options.pipeline_options.GoogleCloudOptions).region = 'us-central1'
    pipeline_options.view_as(
        beam.options.pipeline_options.GoogleCloudOptions).job_name = 'my-job'
    pipeline_options.view_as(
        beam.options.pipeline_options.GoogleCloudOptions).staging_location = 'gs://input-bucket/staging'
    pipeline_options.view_as(
        beam.options.pipeline_options.GoogleCloudOptions).temp_location = 'gs://input-bucket/temp'
    pipeline_options.view_as(beam.options.pipeline_options.StandardOptions).runner = 'DataflowRunner'

    with beam.Pipeline(options=pipeline_options) as p:
        lines = (
                p | 'Read Text File' >> beam.io.ReadFromText(input_path)
        )

        parsed_data = (
                lines | 'Parse CSV' >> beam.FlatMap(parse_csv)
        )

        # Write to BigQuery
        parsed_data | 'Write to BigQuery' >> beam.io.WriteToBigQuery(
            output_table,
            schema='dhc_id:INTEGER,domain_id:INTEGER,domain:STRING,full_name:STRING,city:STRING,state:STRING',
            create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,
            write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND,
            custom_gcs_temp_location='gs://input-bucket/bigquery'
        )

        # Write to GCS
        parsed_data | 'Write to GCS' >> beam.io.WriteToText(output_path)

    logging.info('Job finished!')


if __name__ == '__main__':
    logging.getLogger().setLevel(logging.INFO)
    run()
